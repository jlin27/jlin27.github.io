{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nQuantized Transfer Learning for Computer Vision Tutorial\n========================================================\n\n**Author**: `Zafar Takhirov <https://z-a-f.github.cio>`_\n\n**Reviewed by**: `Raghuraman Krishnamoorthi <https://github.com/raghuramank100>`_\n\n**Edited by**: `Jessica Lin <https://github.com/jlin27>`_\n\nThis tutorial builds on the original `PyTorch Transfer Learning <https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html>`_\ntutorial, written by\n`Sasank Chilamkurthy <https://chsasank.github.io/>`_.\n\nTransfer learning refers to techniques to use a pretrained model for\napplication on a different data-set. Typical scenarios look as follows:\n\n1. **ConvNet as fixed feature extractor**: Here, you \u201cfreeze\u201d[#1]\\_ the\n   weights for all of the network parameters except that of the final\n   several layers (aka \u201cthe head\u201d, usually fully connected layers).\n   These last layers are replaced with new ones initialized with random\n   weights and only these layers are trained.\n2. **Finetuning the convnet**: Instead of random initializaion, you\n   initialize the network with a pretrained network, like the one that\n   is trained on imagenet 1000 dataset. Rest of the training looks as\n   usual. It is common to set the learning rate to a smaller number, as\n   the network is already considered to be trained.\n\nYou can also combine the above two scenarios, and execute them both:\nFirst you can freeze the feature extractor, and train the head. After\nthat, you can unfreeze the feature extractor (or part of it), set the\nlearning rate to something smaller, and continue training.\n\nIn this part you will use the first scenario \u2013 extracting the features\nusing a quantized model.\n\n.. rubric:: Footnotes\n\n.. [#1] \u201cFreezing\u201d the model/layer means running it only in inference\nmode, and not allowing its parameters to be updated during the training.\n\nWe will start by doing the necessary imports:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# imports\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport time\nimport copy\n\nplt.rc('axes', labelsize=18, titlesize=18)\nplt.rc('figure', titlesize=18)\nplt.rc('font', family='DejaVu Sans', serif='Times', size=18)\nplt.rc('legend', fontsize=18)\nplt.rc('lines', linewidth=3)\nplt.rc('text', usetex=False)  # TeX might not be supported\nplt.rc('xtick', labelsize=18)\nplt.rc('ytick', labelsize=18)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Installing the Nightly Build\n----------------------------\n\nBecause you will be using the experimental parts of the PyTorch, it is\nrecommended to install the latest version of ``torch`` and\n``torchvision``. You can find the most recent instructions on local\ninstallation `here <https://pytorch.org/get-started/locally/>`_.\nFor example, to install on Mac:\n\n.. code:: shell\n\n   pip install numpy\n   pip install --pre torch torchvision -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load Data (section not needed as it is covered in the original tutorial)\n------------------------------------------------------------------------\n\nWe will use ``torchvision`` and ``torch.utils.data`` packages to load\nthe data.\n\nThe problem you are going to solve today is classifying **ants** and\n**bees** from images. The dataset contains about 120 training images\neach for ants and bees. There are 75 validation images for each class.\nThis is considered a very small dataset to generalize on. However, since\nwe are using transfer learning, we should be able to generalize\nreasonably well.\n\n*This dataset is a very small subset of imagenet.*\n\n.. Note :: Download the data from\n`here <https://download.pytorch.org/tutorial/hymenoptera_data.zip>`_\nand extract it to the ``data`` directory.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import requests\nimport os\nimport zipfile\n\nDATA_URL = 'https://download.pytorch.org/tutorial/hymenoptera_data.zip'\nDATA_PATH = os.path.join('.', 'data')\nFILE_NAME = os.path.join(DATA_PATH, 'hymenoptera_data.zip')\n\nif not os.path.isfile(FILE_NAME):\n  print(\"Downloading the data...\")\n  os.makedirs('data', exist_ok=True)\n  with requests.get(DATA_URL) as req:\n    with open(FILE_NAME, 'wb') as f:\n      f.write(req.content)\n  if 200 <= req.status_code < 300:\n    print(\"Download complete!\")\n  else:\n    print(\"Download failed!\")\nelse:\n  print(FILE_NAME, \"already exists, skipping download...\")\n\nwith zipfile.ZipFile(FILE_NAME, 'r') as zip_ref:\n  print(\"Unzipping...\")\n  zip_ref.extractall('data')\n\nDATA_PATH = os.path.join(DATA_PATH, 'hymenoptera_data')\n\nimport torch\nfrom torchvision import transforms, datasets\n\n# Data augmentation and normalization for training\n# Just normalization for validation\ndata_transforms = {\n    'train': transforms.Compose([\n        transforms.Resize(224),\n        transforms.RandomCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'val': transforms.Compose([\n        transforms.Resize(224),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}\n\nimage_datasets = {x: datasets.ImageFolder(os.path.join(DATA_PATH, x),\n                                          data_transforms[x])\n                  for x in ['train', 'val']}\ndataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=16,\n                                              shuffle=True, num_workers=8)\n              for x in ['train', 'val']}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\nclass_names = image_datasets['train'].classes\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize a few images\n^^^^^^^^^^^^^^^^^^^^^^\n\nLet\u2019s visualize a few training images so as to understand the data\naugmentations.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torchvision\n\ndef imshow(inp, title=None, ax=None, figsize=(5, 5)):\n  \"\"\"Imshow for Tensor.\"\"\"\n  inp = inp.numpy().transpose((1, 2, 0))\n  mean = np.array([0.485, 0.456, 0.406])\n  std = np.array([0.229, 0.224, 0.225])\n  inp = std * inp + mean\n  inp = np.clip(inp, 0, 1)\n  if ax is None:\n    fig, ax = plt.subplots(1, figsize=figsize)\n  ax.imshow(inp)\n  ax.set_xticks([])\n  ax.set_yticks([])\n  if title is not None:\n    ax.set_title(title)\n\n# Get a batch of training data\ninputs, classes = next(iter(dataloaders['train']))\n\n# Make a grid from batch\nout = torchvision.utils.make_grid(inputs, nrow=4)\n\nfig, ax = plt.subplots(1, figsize=(10, 10))\nimshow(out, title=[class_names[x] for x in classes], ax=ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training the model\n------------------\n\nNow, let\u2019s write a general function to train a model. Here, we will\nillustrate:\n\n-  Scheduling the learning rate\n-  Saving the best model\n\nIn the following, parameter ``scheduler`` is an LR scheduler object from\n``torch.optim.lr_scheduler``.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25, device='cpu'):\n  since = time.time()\n\n  best_model_wts = copy.deepcopy(model.state_dict())\n  best_acc = 0.0\n\n  for epoch in range(num_epochs):\n    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n    print('-' * 10)\n\n    # Each epoch has a training and validation phase\n    for phase in ['train', 'val']:\n      if phase == 'train':\n        model.train()  # Set model to training mode\n      else:\n        model.eval()   # Set model to evaluate mode\n\n      running_loss = 0.0\n      running_corrects = 0\n\n      # Iterate over data.\n      for inputs, labels in dataloaders[phase]:\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward\n        # track history if only in train\n        with torch.set_grad_enabled(phase == 'train'):\n          outputs = model(inputs)\n          _, preds = torch.max(outputs, 1)\n          loss = criterion(outputs, labels)\n\n          # backward + optimize only if in training phase\n          if phase == 'train':\n            loss.backward()\n            optimizer.step()\n\n        # statistics\n        running_loss += loss.item() * inputs.size(0)\n        running_corrects += torch.sum(preds == labels.data)\n      if phase == 'train':\n        scheduler.step()\n\n      epoch_loss = running_loss / dataset_sizes[phase]\n      epoch_acc = running_corrects.double() / dataset_sizes[phase]\n\n      print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n        phase, epoch_loss, epoch_acc))\n\n      # deep copy the model\n      if phase == 'val' and epoch_acc > best_acc:\n        best_acc = epoch_acc\n        best_model_wts = copy.deepcopy(model.state_dict())\n\n    print()\n\n  time_elapsed = time.time() - since\n  print('Training complete in {:.0f}m {:.0f}s'.format(\n    time_elapsed // 60, time_elapsed % 60))\n  print('Best val Acc: {:4f}'.format(best_acc))\n\n  # load best model weights\n  model.load_state_dict(best_model_wts)\n  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualizing the model predictions ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nGeneric function to display predictions for a few images\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def visualize_model(model, rows=3, cols=3):\n  was_training = model.training\n  model.eval()\n  current_row = current_col = 0\n  fig, ax = plt.subplots(rows, cols, figsize=(cols*2, rows*2))\n\n  with torch.no_grad():\n    for idx, (imgs, lbls) in enumerate(dataloaders['val']):\n      imgs = imgs.cpu()\n      lbls = lbls.cpu()\n\n      outputs = model(imgs)\n      _, preds = torch.max(outputs, 1)\n\n      for jdx in range(imgs.size()[0]):\n        imshow(imgs.data[jdx], ax=ax[current_row, current_col])\n        ax[current_row, current_col].axis('off')\n        ax[current_row, current_col].set_title('predicted: {}'.format(class_names[preds[jdx]]))\n\n        current_col += 1\n        if current_col >= cols:\n          current_row += 1\n          current_col = 0\n        if current_row >= rows:\n          model.train(mode=was_training)\n          return\n    model.train(mode=was_training)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Part 1. Training a Custom Classifier based on a Quantized Feature Extractor\n---------------------------------------------------------------------------\n\nIn this section you will use a \u201cfrozen\u201d quantized feature extractor, and\ntrain a custom classifier head on top of it. Unlike floating point\nmodels, you don\u2019t need to set requires_grad=False for the quantized\nmodel, as it has no trainable parameters. Please, refer to the\ndocumentation https://pytorch.org/docs/stable/quantization.html\\ \\_ for\nmore details.\n\nLoad a pretrained model: for this exercise you will be using ResNet-18\nhttps://pytorch.org/hub/pytorch_vision_resnet/\\ \\_.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torchvision.models.quantization as models\n\n# We will need the number of filters in the `fc` for future use.\n# Here the size of each output sample is set to 2.\n# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\nmodel_fe = models.resnet18(pretrained=True, progress=True, quantize=True)\nnum_ftrs = model_fe.fc.in_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "At this point you need to mofify the pretrained model: Because the model\nhas the quantize/dequantize blocks in the beginning and the end, butt we\nwill only uuse the feature extractor, the dequantizatioin layer has to\nmove right before the linear layer (the head). The easiest way of doing\nit is to wrap the model under the ``nn.Sequential``.\n\nThe first step to do, is to isolate the feature extractor in the ResNet\nmodel. Although in this example you are tasked to use all layers except\n``fc`` as the feature extractor, in reality, you can take as many parts\nas you need. This would be useful in case you would like to replace some\nof the convolutional layers as well.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Notice that when isolating the feature extractor from a quantized\nmodel, you have to place the quantizer in the beginning and in the end\nof it.**\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch import nn\n\ndef create_combined_model(model_fe):\n  # Step 1. Isolate the feature extractor.\n  model_fe_features = nn.Sequential(\n    model_fe.quant,  # Quantize the input\n    model_fe.conv1,\n    model_fe.bn1,\n    model_fe.relu,\n    model_fe.maxpool,\n    model_fe.layer1,\n    model_fe.layer2,\n    model_fe.layer3,\n    model_fe.layer4,\n    model_fe.avgpool,\n    model_fe.dequant,  # Dequantize the output\n  )\n\n  # Step 2. Create a new \"head\"\n  new_head = nn.Sequential(\n    nn.Dropout(p=0.5),\n    nn.Linear(num_ftrs, 2),\n  )\n\n  # Step 3. Combine, and don't forget the quant stubs.\n  new_model = nn.Sequential(\n    model_fe_features,\n    nn.Flatten(1),\n    new_head,\n  )\n  return new_model\n\nnew_model = create_combined_model(model_fe)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-danger\"><h4>Warning</h4><p>Currently the quantized models can only be run on CPU.</p></div>\nHowever, it is possible to send the non-quantized parts of the model to\na GPU.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\nnew_model = new_model.to('cpu')\n\ncriterion = nn.CrossEntropyLoss()\n\n# Note that we are only training the head.\noptimizer_ft = optim.SGD(new_model.parameters(), lr=0.01, momentum=0.9)\n\n# Decay LR by a factor of 0.1 every 7 epochs\nexp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train and evaluate\n------------------\n\nThis step takes around 15-25 min on CPU. Because the quantized model can\nonly run on the CPU, you cannot run the training on GPU.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "new_model = train_model(new_model, criterion, optimizer_ft, exp_lr_scheduler,\n                        num_epochs=25, device='cpu')\n\nvisualize_model(new_model)\nplt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Part 2. Finetuning the quantizable model**\n\nIn this part, we fine tune the feature extractor used for transfer\nlearning, and quantize the feature extractor. Note that in both part 1\nand 2, the feature extractor is quantized. The difference is that in\npart 1, we use a pretrained quantized model. In this part, we create a\nquantized feature extractor after fine tuning on the data-set of\ninterest, so this is a way to get better accuracy with transfer learning\nwhile having the benefits of quantization. Note that in our specific\nexample, the training set is really small (120 images) so the benefits\nof fine tuning the entire model is not apparent. However, the procedure\nshown here will improve accuracy for transfer learning with larger\ndatasets.\n\nThe pretrained feature extractor must be quantizable, i.e we need to do\nthe following: 1. Fuse (Conv, BN, ReLU), (Conv, BN) and (Conv, ReLU)\nusing torch.quantization.fuse_modules. 2. Connect the feature extractor\nwith a custom head. This requires dequantizing the output of the feature\nextractor. 3. Insert fake-quantization modules at appropriate locations\nin the feature extractor to mimic quantization during training.\n\nFor step (1), we use models from torchvision/models/quantization, which\nsupport a member method fuse_model, which fuses all the conv, bn, and\nrelu modules. In general, this would require calling the\ntorch.quantization.fuse_modules API with the list of modules to fuse.\n\nStep (2) is done by the function create_custom_model function that we\nused in the previous section.\n\nStep (3) is achieved by using torch.quantization.prepare_qat, which\ninserts fake-quantization modules.\n\nStep (4) Fine tune the model with the desired custom head.\n\nStep (5) We convert the fine tuned model into a quantized model (only\nthe feature extractor is quantized) by calling\ntorch.quantization.convert\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Because of the random initialization your results might differ</p></div>\nfrom the results shown here.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = models.resnet18(pretrained=True, progress=True, quantize=False)  # notice `quantize=False`\nnum_ftrs = model.fc.in_features\n\n# Step 1\nmodel.train()\nmodel.fuse_model()\n# Step 2\nmodel_ft = create_combined_model(model)\nmodel_ft[0].qconfig = torch.quantization.default_qat_qconfig  # Use default QAT configuration\n# Step 3\nmodel_ft = torch.quantization.prepare_qat(model_ft, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finetuning the model\n--------------------\n\nWe fine tune the entire model including the feature extractor. In\ngeneral, this will lead to higher accuracy. However, due to the small\ntraining set used here, we end up overfitting to the training set.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Step 4. Fine tune the model\n\nfor param in model_ft.parameters():\n  param.requires_grad = True\n\nmodel_ft.cuda()  # We can fine-tune on GPU\n\ncriterion = nn.CrossEntropyLoss()\n\n# Note that we are training everything, so the learning rate is lower\n# Notice the smaller learning rate\noptimizer_ft = optim.SGD(model_ft.parameters(), lr=1e-3, momentum=0.9, weight_decay=0.1)\n\n# Decay LR by a factor of 0.3 every several epochs\nexp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=5, gamma=0.3)\n\nmodel_ft_tuned = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n                             num_epochs=25, device='cuda')\n\n# Step 5. Convert to quantized model\n\nfrom torch.quantization import convert\nmodel_ft_tuned.cpu()\n\nmodel_quantized_and_trained = convert(model_ft_tuned, inplace=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets see how the quantized model performs on a few images\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "visualize_model(model_quantized_and_trained)\n\nplt.ioff()\nplt.tight_layout()\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}