{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nSyntaxError\n===========\n\nExample script with invalid Python syntax\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n\"\"\"\n(Experimental) Dynamic Quantization on HuggingFace BERT model\n==============================================================\n**Author**: `Jianyu Huang <https://github.com/jianyuh>`_\n\n**Reviewed by**: `Raghuraman Krishnamoorthi <https://github.com/raghuramank100>`_\n\n**Edited by**: `Jessica Lin <https://github.com/jlin27>`_\n\n\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Introduction\n============\n\nIn this tutorial, we will apply the dynamic quantization on a BERT\nmodel, closely following the BERT model from the HuggingFace\nTransformers examples (https://github.com/huggingface/transformers).\nWith this step-by-step journey, we would like to demonstrate how to\nconvert a well-known state-of-the-art model like BERT into dynamic\nquantized model.\n\n-  BERT, or Bidirectional Embedding Representations from Transformers,\n   is a new method of pre-training language representations which\n   achieves the state-of-the-art accuracy results on many popular\n   Natural Language Processing (NLP) tasks, such as question answering,\n   text classification, and others. The original paper can be found\n   here: https://arxiv.org/pdf/1810.04805.pdf.\n\n-  Dynamic quantization support in PyTorch converts a float model to a\n   quantized model with static int8 or float16 data types for the\n   weights and dynamic quantization for the activations. The activations\n   are quantized dynamically (per batch) to int8 when the weights are\n   quantized to int8.\n\nIn PyTorch, we have ``torch.quantization.quantize_dynamic`` API support\n(https://pytorch.org/docs/stable/quantization.html#torch.quantization.quantize_dynamic),\nwhich replaces specified modules with dynamic weight-only quantized\nversions and output the quantized model.\n\n-  We demonstrate the accuracy and inference performance results on the\n   Microsoft Research Paraphrase Corpus (MRPC) task\n   (https://www.microsoft.com/en-us/download/details.aspx?id=52398) in\n   the General Language Understanding Evaluation benchmark (GLUE)\n   (https://gluebenchmark.com/). The MRPC (Dolan and Brockett, 2005) is\n   a corpus of sentence pairs automatically extracted from online news\n   sources, with human annotations of whether the sentences in the pair\n   are semantically equivalent. Because the classes are imbalanced (68%\n   positive, 32% negative), we follow common practice and report both\n   accuracy and F1 score\n   (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html).\n   MRPC is a common NLP task for language pair classification, as shown\n   below.\n\n.. raw:: html\n\n   <!-- ![BERT for setence pair classification](https://drive.google.com/file/d/1m_VcRJNuMBwnrx3f0OShX6ffLyoEOJPC/view?usp=sharing). -->\n\n.. figure:: https://gluon-nlp.mxnet.io/_images/bert-sentence-pair.png\n   :alt: BERT for setence pair classification\n\n   BERT for setence pair classification\n\n.. raw:: html\n\n   <!-- ![alt text](https://drive.google.com/file/d/1NJIWxtY39pBl0KUCOCMF5vpfuWLlSKf8/view?usp=sharing) -->\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Setup\n=====\n\nInstall PyTorch and HuggingFace Transformers\n--------------------------------------------\n\nTo start this tutorial, let\u2019s first follow the installation instructions\nin PyTorch and HuggingFace Github Repo: -\nhttps://github.com/pytorch/pytorch/#installation -\nhttps://github.com/huggingface/transformers#installation\n\nIn addition, we also install ``sklearn`` package, as we will reuse its\nbuilt-in F1 score calculation helper function.\n\n.. code:: shell\n\n   !pip install sklearn\n   !pip install transformers\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Because we will be using the experimental parts of the PyTorch, it is\nrecommended to install the latest version of torch and torchvision. You\ncan find the most recent instructions on local installation here\nhttps://pytorch.org/get-started/locally/. For example, to install on\nMac:\n\n.. code:: shell\n  !yes y | pip uninstall torch tochvision\n  !yes y | pip install --pre torch -f https://download.pytorch.org/whl/nightly/cu101/torch_nightly.html\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import the necessary modules\n----------------------------\n\nIn this step we import the necessary Python modules for the tutorial.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import, division, print_function\n\nimport logging\nimport numpy as np\nimport os\nimport random\nimport sys\nimport time\nimport torch\n\nfrom argparse import Namespace\nfrom torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n                              TensorDataset)\nfrom tqdm import tqdm\nfrom transformers import (BertConfig, BertForSequenceClassification, BertTokenizer,)\nfrom transformers import glue_compute_metrics as compute_metrics\nfrom transformers import glue_output_modes as output_modes\nfrom transformers import glue_processors as processors\nfrom transformers import glue_convert_examples_to_features as convert_examples_to_features\n\n# Setup logging\nlogger = logging.getLogger(__name__)\nlogging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n                    datefmt = '%m/%d/%Y %H:%M:%S',\n                    level = logging.WARN)\n\nlogging.getLogger(\"transformers.modeling_utils\").setLevel(\n   logging.WARN)  # Reduce logging\n\nprint(torch.__version__)\n# We set the number of threads to compare the single thread performance between FP32 and INT8 performance.\n# In the end of the tutorial, the user can set other number of threads by building PyTorch with right parallel backend.\ntorch.set_num_threads(1)\nprint(torch.__config__.parallel_info())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Download the dataset\n--------------------\n\nBefore running MRPC tasks we download the GLUE data\n(https://gluebenchmark.com/tasks) by running this script\n(https://gist.github.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e,\nhttps://github.com/nyu-mll/GLUE-baselines/blob/master/download_glue_data.py)\nand unpack it to some directory \u201cglue_data/MRPC\u201d.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# !python download_glue_data.py --data_dir='glue_data' --tasks='MRPC' --test_labels=True\n!pwd\n!ls\n!wget https://gist.githubusercontent.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e/raw/17b8dd0d724281ed7c3b2aeeda662b92809aadd5/download_glue_data.py\n!python download_glue_data.py --data_dir='glue_data' --tasks='MRPC'\n!ls glue_data/MRPC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Helper functions\n----------------\n\nThe helper functions are built-in in transformers library. We mainly use\nthe following helper functions: one for converting the text examples\ninto the feature vectors; The other one for measuring the F1 score of\nthe predicted result.\n\nConvert the texts into features\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nglue_convert_examples_to_features (\nhttps://github.com/huggingface/transformers/blob/master/transformers/data/processors/glue.py)\nload a data file into a list of ``InputFeatures``.\n\n-  Tokenize the input sequences;\n-  Insert [CLS] at the beginning;\n-  Insert [SEP] between the first sentence and the second sentence, and\n   at the end;\n-  Generate token type ids to indicate whether a token belongs to the\n   first sequence or the second sequence;\n\nF1 metric\n~~~~~~~~~\n\nThe F1 score\n(https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)\ncan be interpreted as a weighted average of the precision and recall,\nwhere an F1 score reaches its best value at 1 and worst score at 0. The\nrelative contribution of precision and recall to the F1 score are equal.\nThe formula for the F1 score is:\n\nF1 = 2 \\* (precision \\* recall) / (precision + recall)\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fine-tune the BERT model\n========================\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The spirit of BERT is to pre-train the language representations and then\nto fine-tune the deep bi-directional representations on a wide range of\ntasks with minimal task-dependent parameters, and achieves\nstate-of-the-art results. In this tutorial, we will focus on fine-tuning\nwith the pre-trained BERT model to classify semantically equivalent\nsentence pairs on MRPC task.\n\nTo fine-tune the pre-trained BERT model (\u201cbert-base-uncased\u201d model in\nHuggingFace transformers) for the MRPC task, you can follow the command\nin (https://github.com/huggingface/transformers/tree/master/examples):\n\n::\n\n   export GLUE_DIR=./glue_data\n   export TASK_NAME=MRPC\n   export OUT_DIR=/mnt/homedir/jianyuhuang/public/bert/$TASK_NAME/\n   python ./run_glue.py \\\n       --model_type bert \\\n       --model_name_or_path bert-base-uncased \\\n       --task_name $TASK_NAME \\\n       --do_train \\\n       --do_eval \\\n       --do_lower_case \\\n       --data_dir $GLUE_DIR/$TASK_NAME \\\n       --max_seq_length 128 \\\n       --per_gpu_eval_batch_size=8   \\\n       --per_gpu_train_batch_size=8   \\\n       --learning_rate 2e-5 \\\n       --num_train_epochs 3.0 \\\n       --save_steps 100000 \\\n       --output_dir $OUT_DIR\n\nWe provide the fined-tuned BERT model for MRPC task here (We did the\nfine-tuning on CPUs with a total train batch size of 8):\n\nhttps://drive.google.com/drive/folders/1mGBx0t-YJAWXHbgab2f_IimaMiVHlKh-\n\nTo save time, you can manually copy the fined-tuned BERT model for MRPC\ntask in your Google Drive (Create the same \u201cBERT_Quant_Tutorial/MRPC\u201d\nfolder in the Google Drive directory), and then mount your Google Drive\non your runtime using an authorization code, so that we can directly\nread and write the models into Google Drive in the following steps.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\ndrive.mount('/content/drive')\n\n!ls\n!pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set global configurations\n-------------------------\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we set the global configurations for evaluating the fine-tuned BERT\nmodel before and after the dynamic quantization.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "configs = Namespace()\n\n# The output directory for the fine-tuned model.\n# configs.output_dir = \"/mnt/homedir/jianyuhuang/public/bert/MRPC/\"\nconfigs.output_dir = \"/content/drive/My Drive/BERT_Quant_Tutorial/MRPC/\"\n# configs.output_dir = \"./MRPC/\"\n\n# The data directory for the MRPC task in the GLUE benchmark.\n# configs.data_dir = \"/mnt/homedir/jianyuhuang/public/bert/glue_data/MRPC\"\n# configs.data_dir = \"./glue_data/MRPC\"\nconfigs.data_dir = \"/content/glue_data/MRPC\"\n\n# The model name or path for the pre-trained model.\nconfigs.model_name_or_path = \"bert-base-uncased\"\n# The maximum length of an input sequence\nconfigs.max_seq_length = 128\n\n# Prepare GLUE task.\nconfigs.task_name = \"MRPC\".lower()\nconfigs.processor = processors[configs.task_name]()\nconfigs.output_mode = output_modes[configs.task_name]\nconfigs.label_list = configs.processor.get_labels()\nconfigs.model_type = \"bert\".lower()\nconfigs.do_lower_case = True\n\n# Set the device, batch size, topology, and caching flags.\nconfigs.device = \"cpu\"\nconfigs.per_gpu_eval_batch_size = 8\nconfigs.n_gpu = 0\nconfigs.local_rank = -1\nconfigs.overwrite_cache = False\n\n\n# Set random seed for reproducibility.\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\nset_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the fine-tuned BERT model\n------------------------------\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We load the tokenizer and fine-tuned BERT sequence classifier model\n(FP32) from the ``configs.output_dir``.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\n    configs.output_dir, do_lower_case=configs.do_lower_case)\n\nmodel = BertForSequenceClassification.from_pretrained(configs.output_dir)\nmodel.to(configs.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the tokenize and evaluation function\n-------------------------------------------\n\nWe reuse the tokenize and evaluation function from\nhttps://github.com/huggingface/transformers/blob/master/examples/run_glue.py.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\ndef evaluate(args, model, tokenizer, prefix=\"\"):\n    # Loop to handle MNLI double evaluation (matched, mis-matched)\n    eval_task_names = (\"mnli\", \"mnli-mm\") if args.task_name == \"mnli\" else (args.task_name,)\n    eval_outputs_dirs = (args.output_dir, args.output_dir + '-MM') if args.task_name == \"mnli\" else (args.output_dir,)\n\n    results = {}\n    for eval_task, eval_output_dir in zip(eval_task_names, eval_outputs_dirs):\n        eval_dataset = load_and_cache_examples(args, eval_task, tokenizer, evaluate=True)\n\n        if not os.path.exists(eval_output_dir) and args.local_rank in [-1, 0]:\n            os.makedirs(eval_output_dir)\n\n        args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n        # Note that DistributedSampler samples randomly\n        eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n        eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n\n        # multi-gpu eval\n        if args.n_gpu > 1:\n            model = torch.nn.DataParallel(model)\n\n        # Eval!\n        logger.info(\"***** Running evaluation {} *****\".format(prefix))\n        logger.info(\"  Num examples = %d\", len(eval_dataset))\n        logger.info(\"  Batch size = %d\", args.eval_batch_size)\n        eval_loss = 0.0\n        nb_eval_steps = 0\n        preds = None\n        out_label_ids = None\n        for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n            model.eval()\n            batch = tuple(t.to(args.device) for t in batch)\n\n            with torch.no_grad():\n                inputs = {'input_ids':      batch[0],\n                          'attention_mask': batch[1],\n                          'labels':         batch[3]}\n                if args.model_type != 'distilbert':\n                    inputs['token_type_ids'] = batch[2] if args.model_type in ['bert', 'xlnet'] else None  # XLM, DistilBERT and RoBERTa don't use segment_ids\n                outputs = model(**inputs)\n                tmp_eval_loss, logits = outputs[:2]\n\n                eval_loss += tmp_eval_loss.mean().item()\n            nb_eval_steps += 1\n            if preds is None:\n                preds = logits.detach().cpu().numpy()\n                out_label_ids = inputs['labels'].detach().cpu().numpy()\n            else:\n                preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n                out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)\n\n        eval_loss = eval_loss / nb_eval_steps\n        if args.output_mode == \"classification\":\n            preds = np.argmax(preds, axis=1)\n        elif args.output_mode == \"regression\":\n            preds = np.squeeze(preds)\n        result = compute_metrics(eval_task, preds, out_label_ids)\n        results.update(result)\n\n        output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n        with open(output_eval_file, \"w\") as writer:\n            logger.info(\"***** Eval results {} *****\".format(prefix))\n            for key in sorted(result.keys()):\n                logger.info(\"  %s = %s\", key, str(result[key]))\n                writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n\n    return results\n\n\ndef load_and_cache_examples(args, task, tokenizer, evaluate=False):\n    if args.local_rank not in [-1, 0] and not evaluate:\n        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n\n    processor = processors[task]()\n    output_mode = output_modes[task]\n    # Load data features from cache or dataset file\n    cached_features_file = os.path.join(args.data_dir, 'cached_{}_{}_{}_{}'.format(\n        'dev' if evaluate else 'train',\n        list(filter(None, args.model_name_or_path.split('/'))).pop(),\n        str(args.max_seq_length),\n        str(task)))\n    if os.path.exists(cached_features_file) and not args.overwrite_cache:\n        logger.info(\"Loading features from cached file %s\", cached_features_file)\n        features = torch.load(cached_features_file)\n    else:\n        logger.info(\"Creating features from dataset file at %s\", args.data_dir)\n        label_list = processor.get_labels()\n        if task in ['mnli', 'mnli-mm'] and args.model_type in ['roberta']:\n            # HACK(label indices are swapped in RoBERTa pretrained model)\n            label_list[1], label_list[2] = label_list[2], label_list[1]\n        examples = processor.get_dev_examples(args.data_dir) if evaluate else processor.get_train_examples(args.data_dir)\n        features = convert_examples_to_features(examples,\n                                                tokenizer,\n                                                label_list=label_list,\n                                                max_length=args.max_seq_length,\n                                                output_mode=output_mode,\n                                                pad_on_left=bool(args.model_type in ['xlnet']),                 # pad on the left for xlnet\n                                                pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n                                                pad_token_segment_id=4 if args.model_type in ['xlnet'] else 0,\n        )\n        if args.local_rank in [-1, 0]:\n            logger.info(\"Saving features into cached file %s\", cached_features_file)\n            torch.save(features, cached_features_file)\n\n    if args.local_rank == 0 and not evaluate:\n        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n\n    # Convert to Tensors and build dataset\n    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n    all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n    all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n    if output_mode == \"classification\":\n        all_labels = torch.tensor([f.label for f in features], dtype=torch.long)\n    elif output_mode == \"regression\":\n        all_labels = torch.tensor([f.label for f in features], dtype=torch.float)\n\n    dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_labels)\n    return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Apply the dynamic quantization\n==============================\n\nWe call ``torch.quantization.quantize_dynamic`` on the model to apply\nthe dynamic quantization on the HuggingFace BERT model. Specifically,\n\n-  We specify that we want the torch.nn.Linear modules in our model to\n   be quantized;\n-  We specify that we want weights to be converted to quantized int8\n   values.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "quantized_model = torch.quantization.quantize_dynamic(\n    model, {torch.nn.Linear}, dtype=torch.qint8\n)\nprint(quantized_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In PyTorch 1.4 release, we further add the per-channel quantization\nsupport for dynamic quantization.\n\n.. figure:: https://drive.google.com/open?id=1N6P70MR6jJ2tcFnFJ2lROLSFqmiOY--g\n   :alt: Per Tensor Quantization for Weight\n\n   Per Tensor Quantization for Weight\n\n.. figure:: https://drive.google.com/open?id=1nyjUKP5qtkRCJPKtUaXXwhglLMQQ0Dfs\n   :alt: Per Channel Quantization for Weight\n\n   Per Channel Quantization for Weight\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "qconfig_dict = {\n    torch.nn.Linear: torch.quantization.per_channel_dynamic_qconfig\n}\nper_channel_quantized_model = torch.quantization.quantize_dynamic(\n    model, qconfig_dict, dtype=torch.qint8\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check the model size\n--------------------\n\nLet\u2019s first check the model size. We can observe a significant reduction\nin model size:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def print_size_of_model(model):\n    torch.save(model.state_dict(), \"temp.p\")\n    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n    os.remove('temp.p')\n\nprint_size_of_model(model)\nprint_size_of_model(quantized_model)\n# print_size_of_model(per_channel_quantized_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The BERT model used in this tutorial (bert-base-uncased) has a\nvocabulary size V of 30522. With the embedding size of 768, the total\nsize of the word embedding table is ~ 4 (Bytes/FP32) \\* 30522 \\* 768 =\n90 MB. So with the help of quantization, the model size of the\nnon-embedding table part is reduced from 350 MB (FP32 model) to 90 MB\n(INT8 model).\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluate the inference accuracy and time\n----------------------------------------\n\nNext, let\u2019s compare the inference time as well as the evaluation\naccuracy between the original FP32 model and the INT8 model after the\ndynamic quantization.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Evaluate the original FP32 BERT model\ndef time_model_evaluation(model, configs, tokenizer):\n    eval_start_time = time.time()\n    result = evaluate(configs, model, tokenizer, prefix=\"\")\n    eval_end_time = time.time()\n    eval_duration_time = eval_end_time - eval_start_time\n    print(result)\n    print(\"Evaluate total time (seconds): {0:.1f}\".format(eval_duration_time))\n\ntime_model_evaluation(model, configs, tokenizer)\n\n# Evaluate the INT8 BERT model after the dynamic quantization\ntime_model_evaluation(quantized_model, configs, tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Running this locally on a MacBook Pro, without quantization, inference\n(for all 408 examples in MRPC dataset) takes about 160 seconds, and with\nquantization it takes just about 90 seconds. We summarize the results\nfor running the quantized BERT model inference on a Macbook Pro as the\nfollows:\n\n::\n\n   | Prec | F1 score | Model Size | 1 thread | 4 threads |\n   | FP32 |  0.9019  |   438 MB   | 160 sec  | 85 sec    |\n   | INT8 |  0.8953  |   181 MB   |  90 sec  | 46 sec    |\n\nWe have 0.6% F1 score accuracy after applying the post-training dynamic\nquantization on the fine-tuned BERT model on the MRPC task. As a\ncomparison, in the recent paper [3] (Table 1), it achieved 0.8788 by\napplying the post-training dynamic quantization and 0.8956 by applying\nthe quantization-aware training. The main reason is that we support the\nasymmetric quantization in PyTorch while that paper supports the\nsymmetric quantization only.\n\nNote that we set the number of threads to 1 for the single-thread\ncomparison in this tutorial. We also support the intra-op\nparallelization for these quantized INT8 operators. The users can now\nset multi-thread by ``torch.set_num_threads(N)`` (``N`` is the number of\nintra-op parallelization threads). One preliminary requirement to enable\nthe intra-op parallelization support is to build PyTorch with the right\nbackend such as OpenMP, Native, or TBB\n(https://pytorch.org/docs/stable/notes/cpu_threading_torchscript_inference.html#build-options).\nYou can use ``torch.__config__.parallel_info()`` to check the\nparallelization settings. On the same MacBook Pro using PyTorch with\nNative backend for parallelization, we can get about 46 seconds for\nprocessing the evaluation of MRPC dataset.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Evaluate the INT8 BERT model after the per-channel dynamic quantization\ntime_model_evaluation(per_channel_quantized_model, configs, tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Serialize the quantized model\n-----------------------------\n\nWe can serialize and save the quantized model for the future use.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "quantized_output_dir = configs.output_dir + \"quantized/\"\nif not os.path.exists(quantized_output_dir):\n    os.makedirs(quantized_output_dir)\nquantized_model.save_pretrained(quantized_output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Conclusion\n==========\n\nIn this tutorial, we demonstrated how to demonstrate how to convert a\nwell-known state-of-the-art NLP model like BERT into dynamic quantized\nmodel. Dynamic quantization can reduce the size of the model while only\nhaving a limited implication on accuracy.\n\nThanks for reading! As always, we welcome any feedback, so please create\nan issue here (https://github.com/pytorch/pytorch/issues) if you have\nany.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "References\n==========\n\n[1] J.Devlin, M. Chang, K. Lee and K. Toutanova, BERT: Pre-training of\nDeep Bidirectional Transformers for Language Understanding (2018)\n\n[2] HuggingFace Transformers.\nhttps://github.com/huggingface/transformers\n\n[3] O. Zafrir, G. Boudoukh, P. Izsak, & M. Wasserblat (2019). Q8BERT:\nQuantized 8bit BERT. arXiv preprint arXiv:1910.06188.\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}